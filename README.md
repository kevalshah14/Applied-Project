# Natural Language Robot Control Framework

A **transferable framework** for controlling robots using natural language, powered by Large Language Models (LLMs) as planners and executors.

[![GitHub](https://img.shields.io/badge/GitHub-Repository-black.svg)](https://github.com/kevalshah14/Applied-Project)
![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## Demo

[Watch the Project Demo Video](https://github.com/kevalshah14/Applied-Project/blob/master/Report/Applied%20Project%20Demo.mp4)


## Overview

This project enables users to control robotic systems through conversational natural language commands. The core innovation is a **tool abstraction layer** that decouples the LLM's intelligence from robot-specific hardware, making the system easily transferable to different robots.

### Key Features

- ğŸ—£ï¸ **Natural Language Control** â€“ Command robots using plain English
- ğŸ”§ **Transferable Architecture** â€“ Swap robot hardware by only changing tool implementations
- ğŸ‘ï¸ **Open-Vocabulary Object Detection** â€“ Find any object using Gemini VLM
- ğŸ¯ **Precise Segmentation** â€“ SAM 2.1 for accurate object localization
- ğŸ“ **3D Depth Perception** â€“ OAK-D stereo camera for spatial awareness
- ğŸ¤– **LLM as Planner & Executor** â€“ Gemini 2.5 Flash with automatic function calling

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Interface                          â”‚
â”‚                   (Next.js Frontend)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              LLM Agent (Gemini 2.5 Flash)           â”‚    â”‚
â”‚  â”‚         System Prompt + Automatic Tool Calling      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Tool Abstraction Layer                 â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚find_obj  â”‚ â”‚pickup    â”‚ â”‚place     â”‚ â”‚gripper â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Perception  â”‚    â”‚    Control    â”‚    â”‚   External    â”‚
â”‚   (OAK-D +    â”‚    â”‚   (Dobot +    â”‚    â”‚   AI APIs     â”‚
â”‚    SAM 2.1)   â”‚    â”‚   pydobot)    â”‚    â”‚   (Gemini)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
Applied-Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ state.py             # Shared state (image store)
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ chat.py          # LLM chat with tool configuration
â”‚   â”‚   â””â”€â”€ tools.py         # Robot tool definitions
â”‚   â”œâ”€â”€ perception/
â”‚   â”‚   â”œâ”€â”€ stream.py        # Camera streaming manager
â”‚   â”‚   â”œâ”€â”€ depth.py         # Depth estimation utilities
â”‚   â”‚   â””â”€â”€ init.py          # DepthAI pipeline setup
â”‚   â”œâ”€â”€ controls/
â”‚   â”‚   â”œâ”€â”€ __init__.py      # Robot controller export
â”‚   â”‚   â””â”€â”€ robot_control.py # Dobot hardware interface
â”‚   â””â”€â”€ data_collection/     # Data collection for ACT fine-tuning
â”‚       â”œâ”€â”€ __init__.py      # Module exports
â”‚       â”œâ”€â”€ collect_data.py  # Record demonstrations in HDF5
â”‚       â””â”€â”€ convert_to_lerobot.py  # Convert to LeRobot format
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/             # Next.js app router pages
â”‚   â”‚   â””â”€â”€ components/      # React components
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”œâ”€â”€ Report.tex               # Academic paper
â””â”€â”€ README.md
```

## Hardware Requirements

- **Robot Arm**: Dobot Magician (4-DOF) with suction gripper
- **Camera**: Luxonis OAK-D / OAK-D Lite stereo camera
- **Computer**: Any machine capable of running Python 3.11+ and Node.js 18+

## Software Requirements

- Python 3.11+
- Node.js 18+
- Google Gemini API key

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/kevalshah14/Applied-Project.git
cd Applied-Project
```

### 2. Backend Setup

```bash
cd backend

# Create virtual environment (using uv recommended)
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
uv sync

# Or with pip
pip install -r requirements.txt
```

### 3. Environment Variables

Create a `.env` file in the `backend/` directory:

```env
APIKEY=your_google_gemini_api_key
```

### 4. Frontend Setup

```bash
cd frontend
npm install
```

## Usage

### 1. Start the Backend

```bash
cd backend
python main.py
# or
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 2. Start the Frontend

```bash
cd frontend
npm run dev
```

### 3. Open the Interface

Navigate to [http://localhost:3000](http://localhost:3000) in your browser.

### 4. Example Commands

| Command | Description |
|---------|-------------|
| "Show me the camera" | Activates live camera stream |
| "Where is the apple?" | Finds and returns 3D coordinates |
| "Pick up the banana" | Executes full pick sequence |
| "Place it in the box" | Moves to target and releases |
| "Go home" | Returns robot to home position |
| "Open the gripper" | Releases suction |

## Defined Tools

| Tool | Description |
|------|-------------|
| `access_camera` | Activates camera stream for visualization |
| `find_object(description)` | Locates object and returns 3D coordinates (mm) |
| `get_depth(x, y)` | Returns depth at a specific 2D point |
| `pickup_object(x, y, z)` | Executes pick sequence: approach, lower, grasp, lift |
| `place_object(x, y, z)` | Moves to target location and releases gripper |
| `open_gripper()` | Opens gripper / releases suction |
| `close_gripper()` | Closes gripper / activates suction |
| `go_home()` | Moves robot to safe home position |
| `get_robot_pose()` | Returns current end-effector pose |

## Transferability

The key design principle is **hardware abstraction**. To adapt this system to a different robot:

1. **Replace `robot_control.py`** â€“ Implement the same interface for your robot
2. **Update gripper tools** â€“ Match your gripper's API
3. **Re-calibrate** â€“ Adjust camera-to-robot transformation

**What stays the same:**
- LLM prompts and tool descriptions
- Frontend interface
- Perception pipeline (Gemini + SAM)

## Data Collection for ACT Fine-tuning

This project includes a data collection pipeline that enables **progressive improvement** through imitation learning. The LLM initially uses tools for manipulation, then transitions to invoking trained **ACT** policies once sufficient demonstration data is collected.

### Progressive Learning Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 1: Tool-Based Execution                â”‚
â”‚  User: "Pick up the red cube"                                   â”‚
â”‚  LLM â†’ find_object() â†’ pickup_object() â†’ [Success recorded]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [50+ demonstrations collected]
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 2: Policy Training                     â”‚
â”‚  HDF5 episodes â†’ LeRobot format â†’ Fine-tune ACT                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 3: Policy Invocation                   â”‚
â”‚  User: "Pick up the red cube"                                   â”‚
â”‚  LLM â†’ execute_policy("pick_red_cube") â†’ [Faster execution]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Capabilities

| Phase | Approach | Speed | Flexibility |
|-------|----------|-------|-------------|
| Initial | Tool-based | Slower | High (any task) |
| Learned | Policy-based | Fast | Task-specific |
| Hybrid | LLM decides | Optimal | Best of both |

### Collecting Demonstrations

```bash
cd backend/data_collection

# Collect 50 episodes for a pick-and-place task
python collect_data.py \
    --task "pick_red_cube" \
    --num-episodes 50 \
    --output-dir data/dobot/demonstrations
```

**During collection:**
1. Press Enter when ready to start each episode
2. Physically guide the robot through the demonstration (kinesthetic teaching)
3. Press Ctrl+C when the demonstration is complete
4. Repeat for all episodes

### Converting to LeRobot Format

```bash
python convert_to_lerobot.py \
    --input-dir data/dobot/demonstrations \
    --repo-id your_username/dobot_pick_cube \
    --task "pick red cube"
```

**Options:**
- `--push-to-hub` â€“ Upload dataset to HuggingFace Hub
- `--mode video` â€“ Use video compression for smaller files

### Data Format

Each episode records:
| Data | Shape | Description |
|------|-------|-------------|
| Robot State | (5,) | x, y, z, r, suction |
| Image | (224, 224, 3) | RGB from OAK-D camera |
| Action | (5,) | Change in state (delta) |

### Training ACT Policies

After collecting and converting data, train ACT policies:

```bash
# See: https://github.com/huggingface/lerobot
# ACT training example:
python lerobot/scripts/train.py \
    policy=act \
    env=your_env \
    dataset_repo_id=your_username/dobot_pick_cube
```

### LLM as Meta-Controller

Once policies are trained, the LLM evolves from executor to orchestrator:

- **Novel tasks** â†’ LLM uses tools (`find_object`, `pickup_object`, etc.)
- **Learned tasks** â†’ LLM invokes policy (`execute_policy("pick_red_cube")`)
- **Hybrid tasks** â†’ LLM uses tools for novel parts, policies for learned parts

This creates a **self-improving system** where the robot becomes progressively more capable while maintaining flexibility for new situations.

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/chat` | POST | Send message, receive streamed response |
| `/health` | GET | Health check |
| `/image/{id}` | GET | Retrieve annotated image by ID |
| `/stream` | GET | Live camera MJPEG stream |

## Technology Stack

### Backend
- **FastAPI** â€“ Async web framework
- **Google Gemini** â€“ LLM with function calling
- **Ultralytics SAM 2.1** â€“ Segmentation
- **DepthAI** â€“ OAK-D camera SDK
- **pydobot** â€“ Dobot control library
- **OpenCV** â€“ Image processing

### Frontend
- **Next.js 14** â€“ React framework with App Router
- **TypeScript** â€“ Type safety
- **Tailwind CSS** â€“ Styling

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.

## License

This project is licensed under the MIT License.

## Acknowledgments

- Google DeepMind for Gemini
- Meta AI for Segment Anything Model
- Luxonis for DepthAI SDK
- Dobot for pydobot library
- Stanford/Google for ACT (Action Chunking with Transformers)
- HuggingFace for LeRobot framework
